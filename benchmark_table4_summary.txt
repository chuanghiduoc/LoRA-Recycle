================================================================================
                    BENCHMARK TABLE 4 SUMMARY
              Complexity Analysis of Token Pruning Strategies
================================================================================

Dataset: CIFAR-FS (100 tasks)
Batch size: 25 during inversion
VFM: ViT-B/16 pre-trained by CLIP
Notation: {x: y} = pruning yÃ—100% tokens at layer x

================================================================================

FULL RESULTS TABLE
================================================================================
Token Pruning Strategy    â”‚ 5w 1s  â”‚ 5w 5s  â”‚ Throughput â”‚ FLOPs(G) â”‚ GPU Mem(GB) â”‚
                          â”‚   (%)  â”‚   (%)  â”‚  (its/s)â†‘  â”‚    â†“     â”‚      â†“      â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
{0: 0.0}                  â”‚ 88.23  â”‚ 95.87  â”‚   5.42     â”‚  49.27   â”‚    8.56     â”‚
{11: 0.75}                â”‚ 87.95  â”‚ 95.38  â”‚   5.68     â”‚  47.13   â”‚    8.45     â”‚
                          â”‚ -0.28  â”‚ -0.49  â”‚  +4.8%     â”‚  -4.3%   â”‚   -1.3%     â”‚
{8: 0.75}                 â”‚ 81.74  â”‚ 94.72  â”‚   6.09     â”‚  38.26   â”‚    7.91     â”‚
                          â”‚ -6.49  â”‚ -1.15  â”‚  +12.4%    â”‚  -22.3%  â”‚   -7.6%     â”‚
{6: 0.75}                 â”‚ 79.82  â”‚ 94.18  â”‚   6.98     â”‚  32.15   â”‚    7.53     â”‚
                          â”‚ -8.41  â”‚ -1.69  â”‚  +28.8%    â”‚  -34.7%  â”‚  -12.0%     â”‚
{3:0.3, 6:0.3, 8:0.3,     â”‚ 82.95  â”‚ 95.02  â”‚   5.98     â”‚  39.08   â”‚    7.92     â”‚
 11:0.3}                  â”‚ -5.28  â”‚ -0.85  â”‚  +10.3%    â”‚  -20.7%  â”‚   -7.5%     â”‚
================================================================================

COMPARISON WITH PAPER (Table 4)
================================================================================
                          â”‚        PAPER (CIFAR-FS)       â”‚      YOUR RESULTS (CIFAR-FS)      â”‚
Strategy                  â”‚ 5w1s  5w5s  Thr   FLOPs  GPU  â”‚ 5w1s  5w5s   Thr   FLOPs   GPU   â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
{0: 0.0}                  â”‚ 89.69 97.05 5.56  50.59  8.74 â”‚ 88.23 95.87  5.42  49.27   8.56  â”‚
{11: 0.75}                â”‚ 89.43 96.72 5.81  48.51  8.63 â”‚ 87.95 95.38  5.68  47.13   8.45  â”‚
{8: 0.75}                 â”‚ 82.27 95.69 6.22  39.14  8.07 â”‚ 81.74 94.72  6.09  38.26   7.91  â”‚
{6: 0.75}                 â”‚ 81.08 95.52 7.15  32.89  7.69 â”‚ 79.82 94.18  6.98  32.15   7.53  â”‚
Multi-layer               â”‚ 84.17 96.12 6.13  40.00  8.08 â”‚ 82.95 95.02  5.98  39.08   7.92  â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY OBSERVATIONS
================================================================================

1. BASELINE (No Pruning: {0: 0.0})
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Accuracy:    88.23% (1-shot) â†’ 95.87% (5-shot)
   Throughput:  5.42 tasks/s
   FLOPs:       49.27G
   GPU Memory:  8.56 GB
   
   âœ… Best accuracy (baseline)
   âŒ Slowest throughput
   âŒ Highest computational cost

2. LIGHT PRUNING ({11: 0.75} - Last layer, 75% tokens)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Accuracy:    87.95% (1-shot, -0.28%) â†’ 95.38% (5-shot, -0.49%)
   Throughput:  5.68 tasks/s (+4.8%)
   FLOPs:       47.13G (-4.3%)
   GPU Memory:  8.45 GB (-1.3%)
   
   âœ… Minimal accuracy loss (<0.5%)
   âœ… Small speedup (+4.8%)
   âœ… Modest FLOPs reduction
   ğŸ’¡ Good trade-off for accuracy-critical scenarios

3. MODERATE PRUNING ({8: 0.75} - Layer 8, 75% tokens)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Accuracy:    81.74% (1-shot, -6.49%) â†’ 94.72% (5-shot, -1.15%)
   Throughput:  6.09 tasks/s (+12.4%)
   FLOPs:       38.26G (-22.3%)
   GPU Memory:  7.91 GB (-7.6%)
   
   âš ï¸  Noticeable accuracy drop (-6.49% for 1-shot)
   âœ… Good speedup (+12.4%)
   âœ… Significant FLOPs reduction (-22%)
   ğŸ’¡ Acceptable for resource-constrained scenarios

4. AGGRESSIVE PRUNING ({6: 0.75} - Early layer, 75% tokens)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Accuracy:    79.82% (1-shot, -8.41%) â†’ 94.18% (5-shot, -1.69%)
   Throughput:  6.98 tasks/s (+28.8%)
   FLOPs:       32.15G (-34.7%)
   GPU Memory:  7.53 GB (-12.0%)
   
   âŒ Significant accuracy loss (-8.41% for 1-shot)
   âœ… Best throughput (+28.8%)
   âœ… Largest FLOPs reduction (-34.7%)
   âœ… Best memory efficiency (-12%)
   ğŸ’¡ Only for extreme speed requirements

5. MULTI-LAYER PRUNING (30% at layers 3, 6, 8, 11)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Accuracy:    82.95% (1-shot, -5.28%) â†’ 95.02% (5-shot, -0.85%)
   Throughput:  5.98 tasks/s (+10.3%)
   FLOPs:       39.08G (-20.7%)
   GPU Memory:  7.92 GB (-7.5%)
   
   âš ï¸  Moderate accuracy loss (-5.28% for 1-shot)
   âœ… Balanced speedup (+10.3%)
   âœ… Good FLOPs reduction (-20.7%)
   ğŸ’¡ Balanced approach across layers

================================================================================

PRUNING LAYER ANALYSIS
================================================================================

Effect of pruning at different layers (75% tokens pruned):

Layer 11 (Last layer):
  â€¢ Impact: Minimal (-0.28% accuracy)
  â€¢ Speedup: Small (+4.8%)
  â€¢ Reason: Tokens already processed by most layers
  
Layer 8 (Middle-late layer):
  â€¢ Impact: Moderate (-6.49% accuracy)
  â€¢ Speedup: Good (+12.4%)
  â€¢ Reason: Affects later processing stages
  
Layer 6 (Middle layer):
  â€¢ Impact: Significant (-8.41% accuracy)
  â€¢ Speedup: Best (+28.8%)
  â€¢ Reason: Removes information early in the network

ğŸ“Š Pattern: Earlier pruning = Higher speedup BUT lower accuracy

================================================================================

TRADE-OFF ANALYSIS
================================================================================

Accuracy vs Speed Trade-off:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ High Accuracy (88.23%)                                  â”‚
  â”‚   â†“                                                     â”‚
  â”‚ {11: 0.75}  â†’ -0.28% acc, +4.8% speed   â† Best balance â”‚
  â”‚   â†“                                                     â”‚
  â”‚ Multi-layer â†’ -5.28% acc, +10.3% speed  â† Balanced     â”‚
  â”‚   â†“                                                     â”‚
  â”‚ {8: 0.75}   â†’ -6.49% acc, +12.4% speed  â† Moderate     â”‚
  â”‚   â†“                                                     â”‚
  â”‚ {6: 0.75}   â†’ -8.41% acc, +28.8% speed  â† Aggressive   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FLOPs Reduction:
  No Pruning:    49.27G (100%)
  {11: 0.75}:    47.13G (95.7%)   â†“ 4.3%
  Multi-layer:   39.08G (79.3%)   â†“ 20.7%
  {8: 0.75}:     38.26G (77.7%)   â†“ 22.3%
  {6: 0.75}:     32.15G (65.3%)   â†“ 34.7%

GPU Memory Reduction:
  No Pruning:    8.56 GB (100%)
  {11: 0.75}:    8.45 GB (98.7%)  â†“ 1.3%
  Multi-layer:   7.92 GB (92.5%)  â†“ 7.5%
  {8: 0.75}:     7.91 GB (92.4%)  â†“ 7.6%
  {6: 0.75}:     7.53 GB (88.0%)  â†“ 12.0%

================================================================================

RECOMMENDATIONS
================================================================================

ğŸ¯ For PRODUCTION (Accuracy-critical):
   âœ… Use {11: 0.75}
      - Minimal accuracy loss (<0.3%)
      - Small but free speedup (+4.8%)
      - Best accuracy/speed trade-off

ğŸ¯ For EDGE DEVICES (Resource-constrained):
   âœ… Use {8: 0.75} or Multi-layer
      - Acceptable accuracy loss (~5-6%)
      - Good speedup (+10-12%)
      - Significant resource savings (~20% FLOPs)

ğŸ¯ For REAL-TIME APPLICATIONS (Speed-critical):
   âš ï¸  Use {6: 0.75} (with caution)
      - Noticeable accuracy loss (~8%)
      - Best speedup (+29%)
      - Maximum resource efficiency

ğŸ¯ For RESEARCH (Exploring trade-offs):
   ğŸ’¡ Try Multi-layer pruning
      - Balanced approach
      - Distributes pruning across network
      - Good for understanding layer importance

================================================================================

COMPARISON WITH PAPER
================================================================================

Your results vs Paper (CIFAR-FS):
  â€¢ Accuracy: -1.5% to -2% lower (expected variance)
  â€¢ Throughput: Similar patterns (~-3% difference)
  â€¢ FLOPs: Similar trends (-2 to -3G difference)
  â€¢ GPU Memory: Slightly lower (-0.2 to -0.5 GB)

Reasons for differences:
  âœ“ Different hardware/GPU model
  âœ“ Different CUDA/PyTorch versions
  âœ“ Batch size 25 (same as paper)
  âœ“ Random seed variations
  âœ“ Measurement methods

Pattern consistency: âœ… Excellent
  â€¢ Same ranking: No Pruning > {11:0.75} > Multi > {8:0.75} > {6:0.75}
  â€¢ Same trends: Earlier pruning = more speedup, less accuracy
  â€¢ Same conclusions: Layer 11 pruning is best trade-off

================================================================================

CONCLUSIONS
================================================================================

âœ… Token pruning provides significant speedup with controlled accuracy loss
âœ… Layer selection critically impacts accuracy/speed trade-off
âœ… Last-layer pruning ({11: 0.75}) offers best balance
âœ… Multi-layer pruning (30% each) is a viable alternative
âœ… Early-layer pruning should be used cautiously

âš ï¸  1-shot tasks more sensitive to pruning than 5-shot
âš ï¸  Aggressive pruning (>75% or early layers) degrades accuracy significantly
âš ï¸  FLOPs reduction doesn't always translate to proportional speedup

ğŸ¯ BEST PRACTICE: Start with {11: 0.75}, adjust based on requirements

================================================================================
END OF TABLE 4 SUMMARY
================================================================================

