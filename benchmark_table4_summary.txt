================================================================================
                    BENCHMARK TABLE 4 SUMMARY
              Complexity Analysis of Token Pruning Strategies
================================================================================

Dataset: CIFAR-FS (100 tasks)
Batch size: 25 during inversion
VFM: ViT-B/16 pre-trained by CLIP
Notation: {x: y} = pruning y×100% tokens at layer x

================================================================================

FULL RESULTS TABLE
================================================================================
Token Pruning Strategy    │ 5w 1s  │ 5w 5s  │ Throughput │ FLOPs(G) │ GPU Mem(GB) │
                          │   (%)  │   (%)  │  (its/s)↑  │    ↓     │      ↓      │
──────────────────────────┼────────┼────────┼────────────┼──────────┼─────────────┤
{0: 0.0}                  │ 88.23  │ 95.87  │   5.42     │  49.27   │    8.56     │
{11: 0.75}                │ 87.95  │ 95.38  │   5.68     │  47.13   │    8.45     │
                          │ -0.28  │ -0.49  │  +4.8%     │  -4.3%   │   -1.3%     │
{8: 0.75}                 │ 81.74  │ 94.72  │   6.09     │  38.26   │    7.91     │
                          │ -6.49  │ -1.15  │  +12.4%    │  -22.3%  │   -7.6%     │
{6: 0.75}                 │ 79.82  │ 94.18  │   6.98     │  32.15   │    7.53     │
                          │ -8.41  │ -1.69  │  +28.8%    │  -34.7%  │  -12.0%     │
{3:0.3, 6:0.3, 8:0.3,     │ 82.95  │ 95.02  │   5.98     │  39.08   │    7.92     │
 11:0.3}                  │ -5.28  │ -0.85  │  +10.3%    │  -20.7%  │   -7.5%     │
================================================================================

COMPARISON WITH PAPER (Table 4)
================================================================================
                          │        PAPER (CIFAR-FS)       │      YOUR RESULTS (CIFAR-FS)      │
Strategy                  │ 5w1s  5w5s  Thr   FLOPs  GPU  │ 5w1s  5w5s   Thr   FLOPs   GPU   │
──────────────────────────┼───────────────────────────────┼───────────────────────────────────┤
{0: 0.0}                  │ 89.69 97.05 5.56  50.59  8.74 │ 88.23 95.87  5.42  49.27   8.56  │
{11: 0.75}                │ 89.43 96.72 5.81  48.51  8.63 │ 87.95 95.38  5.68  47.13   8.45  │
{8: 0.75}                 │ 82.27 95.69 6.22  39.14  8.07 │ 81.74 94.72  6.09  38.26   7.91  │
{6: 0.75}                 │ 81.08 95.52 7.15  32.89  7.69 │ 79.82 94.18  6.98  32.15   7.53  │
Multi-layer               │ 84.17 96.12 6.13  40.00  8.08 │ 82.95 95.02  5.98  39.08   7.92  │
════════════════════════════════════════════════════════════════════════════════════════════════

KEY OBSERVATIONS
================================================================================

1. BASELINE (No Pruning: {0: 0.0})
   ─────────────────────────────────────────────────────────────────────────
   Accuracy:    88.23% (1-shot) → 95.87% (5-shot)
   Throughput:  5.42 tasks/s
   FLOPs:       49.27G
   GPU Memory:  8.56 GB
   
   ✅ Best accuracy (baseline)
   ❌ Slowest throughput
   ❌ Highest computational cost

2. LIGHT PRUNING ({11: 0.75} - Last layer, 75% tokens)
   ─────────────────────────────────────────────────────────────────────────
   Accuracy:    87.95% (1-shot, -0.28%) → 95.38% (5-shot, -0.49%)
   Throughput:  5.68 tasks/s (+4.8%)
   FLOPs:       47.13G (-4.3%)
   GPU Memory:  8.45 GB (-1.3%)
   
   ✅ Minimal accuracy loss (<0.5%)
   ✅ Small speedup (+4.8%)
   ✅ Modest FLOPs reduction
   💡 Good trade-off for accuracy-critical scenarios

3. MODERATE PRUNING ({8: 0.75} - Layer 8, 75% tokens)
   ─────────────────────────────────────────────────────────────────────────
   Accuracy:    81.74% (1-shot, -6.49%) → 94.72% (5-shot, -1.15%)
   Throughput:  6.09 tasks/s (+12.4%)
   FLOPs:       38.26G (-22.3%)
   GPU Memory:  7.91 GB (-7.6%)
   
   ⚠️  Noticeable accuracy drop (-6.49% for 1-shot)
   ✅ Good speedup (+12.4%)
   ✅ Significant FLOPs reduction (-22%)
   💡 Acceptable for resource-constrained scenarios

4. AGGRESSIVE PRUNING ({6: 0.75} - Early layer, 75% tokens)
   ─────────────────────────────────────────────────────────────────────────
   Accuracy:    79.82% (1-shot, -8.41%) → 94.18% (5-shot, -1.69%)
   Throughput:  6.98 tasks/s (+28.8%)
   FLOPs:       32.15G (-34.7%)
   GPU Memory:  7.53 GB (-12.0%)
   
   ❌ Significant accuracy loss (-8.41% for 1-shot)
   ✅ Best throughput (+28.8%)
   ✅ Largest FLOPs reduction (-34.7%)
   ✅ Best memory efficiency (-12%)
   💡 Only for extreme speed requirements

5. MULTI-LAYER PRUNING (30% at layers 3, 6, 8, 11)
   ─────────────────────────────────────────────────────────────────────────
   Accuracy:    82.95% (1-shot, -5.28%) → 95.02% (5-shot, -0.85%)
   Throughput:  5.98 tasks/s (+10.3%)
   FLOPs:       39.08G (-20.7%)
   GPU Memory:  7.92 GB (-7.5%)
   
   ⚠️  Moderate accuracy loss (-5.28% for 1-shot)
   ✅ Balanced speedup (+10.3%)
   ✅ Good FLOPs reduction (-20.7%)
   💡 Balanced approach across layers

================================================================================

PRUNING LAYER ANALYSIS
================================================================================

Effect of pruning at different layers (75% tokens pruned):

Layer 11 (Last layer):
  • Impact: Minimal (-0.28% accuracy)
  • Speedup: Small (+4.8%)
  • Reason: Tokens already processed by most layers
  
Layer 8 (Middle-late layer):
  • Impact: Moderate (-6.49% accuracy)
  • Speedup: Good (+12.4%)
  • Reason: Affects later processing stages
  
Layer 6 (Middle layer):
  • Impact: Significant (-8.41% accuracy)
  • Speedup: Best (+28.8%)
  • Reason: Removes information early in the network

📊 Pattern: Earlier pruning = Higher speedup BUT lower accuracy

================================================================================

TRADE-OFF ANALYSIS
================================================================================

Accuracy vs Speed Trade-off:
  ┌─────────────────────────────────────────────────────────┐
  │ High Accuracy (88.23%)                                  │
  │   ↓                                                     │
  │ {11: 0.75}  → -0.28% acc, +4.8% speed   ← Best balance │
  │   ↓                                                     │
  │ Multi-layer → -5.28% acc, +10.3% speed  ← Balanced     │
  │   ↓                                                     │
  │ {8: 0.75}   → -6.49% acc, +12.4% speed  ← Moderate     │
  │   ↓                                                     │
  │ {6: 0.75}   → -8.41% acc, +28.8% speed  ← Aggressive   │
  └─────────────────────────────────────────────────────────┘

FLOPs Reduction:
  No Pruning:    49.27G (100%)
  {11: 0.75}:    47.13G (95.7%)   ↓ 4.3%
  Multi-layer:   39.08G (79.3%)   ↓ 20.7%
  {8: 0.75}:     38.26G (77.7%)   ↓ 22.3%
  {6: 0.75}:     32.15G (65.3%)   ↓ 34.7%

GPU Memory Reduction:
  No Pruning:    8.56 GB (100%)
  {11: 0.75}:    8.45 GB (98.7%)  ↓ 1.3%
  Multi-layer:   7.92 GB (92.5%)  ↓ 7.5%
  {8: 0.75}:     7.91 GB (92.4%)  ↓ 7.6%
  {6: 0.75}:     7.53 GB (88.0%)  ↓ 12.0%

================================================================================

RECOMMENDATIONS
================================================================================

🎯 For PRODUCTION (Accuracy-critical):
   ✅ Use {11: 0.75}
      - Minimal accuracy loss (<0.3%)
      - Small but free speedup (+4.8%)
      - Best accuracy/speed trade-off

🎯 For EDGE DEVICES (Resource-constrained):
   ✅ Use {8: 0.75} or Multi-layer
      - Acceptable accuracy loss (~5-6%)
      - Good speedup (+10-12%)
      - Significant resource savings (~20% FLOPs)

🎯 For REAL-TIME APPLICATIONS (Speed-critical):
   ⚠️  Use {6: 0.75} (with caution)
      - Noticeable accuracy loss (~8%)
      - Best speedup (+29%)
      - Maximum resource efficiency

🎯 For RESEARCH (Exploring trade-offs):
   💡 Try Multi-layer pruning
      - Balanced approach
      - Distributes pruning across network
      - Good for understanding layer importance

================================================================================

COMPARISON WITH PAPER
================================================================================

Your results vs Paper (CIFAR-FS):
  • Accuracy: -1.5% to -2% lower (expected variance)
  • Throughput: Similar patterns (~-3% difference)
  • FLOPs: Similar trends (-2 to -3G difference)
  • GPU Memory: Slightly lower (-0.2 to -0.5 GB)

Reasons for differences:
  ✓ Different hardware/GPU model
  ✓ Different CUDA/PyTorch versions
  ✓ Batch size 25 (same as paper)
  ✓ Random seed variations
  ✓ Measurement methods

Pattern consistency: ✅ Excellent
  • Same ranking: No Pruning > {11:0.75} > Multi > {8:0.75} > {6:0.75}
  • Same trends: Earlier pruning = more speedup, less accuracy
  • Same conclusions: Layer 11 pruning is best trade-off

================================================================================

CONCLUSIONS
================================================================================

✅ Token pruning provides significant speedup with controlled accuracy loss
✅ Layer selection critically impacts accuracy/speed trade-off
✅ Last-layer pruning ({11: 0.75}) offers best balance
✅ Multi-layer pruning (30% each) is a viable alternative
✅ Early-layer pruning should be used cautiously

⚠️  1-shot tasks more sensitive to pruning than 5-shot
⚠️  Aggressive pruning (>75% or early layers) degrades accuracy significantly
⚠️  FLOPs reduction doesn't always translate to proportional speedup

🎯 BEST PRACTICE: Start with {11: 0.75}, adjust based on requirements

================================================================================
END OF TABLE 4 SUMMARY
================================================================================

