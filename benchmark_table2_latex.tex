\documentclass{article}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{margin=1in}

\begin{document}

\begin{table}[ht]
\centering
\caption{Recycle in-domain LoRAs. The VFM utilizes ViT-B/16 pre-trained by CLIP. \textbf{FT} refers to fine-tuning-based baselines and \textbf{FTF} refers to fine-tuning-free baselines. \textbf{LoRA Recycle}$_x$ indicates using $x$\% token-masked images for meta-training.}
\label{tab:in_domain_loras}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{MiniImageNet}} & \multicolumn{2}{c}{\textbf{VGG-Flower}} & \multicolumn{2}{c}{\textbf{CUB}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& \textbf{1-shot} & \textbf{5-shot} & \textbf{1-shot} & \textbf{5-shot} & \textbf{1-shot} & \textbf{5-shot} \\
\midrule
\multicolumn{7}{l}{\textbf{FT}} \\
\quad Full Finetuning & 22.47 & 24.18 & 24.35 & 32.67 & 22.69 & 25.93 \\
\midrule
\multicolumn{7}{l}{\textbf{FTF}} \\
\quad \textbf{LoRA Recycle} & 87.23 & 94.87 & 91.28 & 97.45 & 89.74 & 96.12 \\
\quad \textbf{LoRA Recycle}$_{\mathbf{25}}$ & \textbf{89.52} & 95.38 & 92.71 & \textbf{97.89} & 88.95 & 95.87 \\
\quad \textbf{LoRA Recycle}$_{\mathbf{50}}$ & 88.74 & \textbf{95.12} & \textbf{93.05} & 97.72 & 89.38 & 96.45 \\
\quad \textbf{LoRA Recycle}$_{\mathbf{75}}$ & 88.19 & 94.83 & 92.87 & 97.51 & \textbf{90.85} & \textbf{97.28} \\
\bottomrule
\end{tabular}
\end{table}

\end{document}

